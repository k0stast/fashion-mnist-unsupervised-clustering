# -*- coding: utf-8 -*-
"""Εργασία 3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wBH970ggkunApt2-h7YCwRLWntgZwB6_
"""

# Step 1: Import necessary libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import time
import random

from tensorflow.keras.datasets import fashion_mnist
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load the Fashion-MNIST dataset from Keras
(x_train_full, y_train_full), (x_test, y_test) = fashion_mnist.load_data()

# Print dataset shapes to verify correct loading
print(f"Train set shape: {x_train_full.shape}, Labels: {y_train_full.shape}")
print(f"Test set shape: {x_test.shape}, Labels: {y_test.shape}")

# Step 2: Prepare and split the dataset
import numpy as np
from sklearn.model_selection import train_test_split

# Normalize the pixel values to [0, 1]
x_train_full = x_train_full.astype('float32') / 255.0
x_test = x_test.astype('float32') / 255.0

# Flatten the 28x28 images to 784-dimensional vectors
x_train_full = x_train_full.reshape(-1, 28 * 28)
x_test = x_test.reshape(-1, 28 * 28)

# Split the original training set into training and validation sets (80%-20%)
x_train, x_val, y_train, y_val = train_test_split(
    x_train_full, y_train_full, test_size=0.2, random_state=42
)

# Print final shapes
print(f"x_train shape: {x_train.shape}")
print(f"x_val shape: {x_val.shape}")
print(f"x_test shape: {x_test.shape}")

# Step 3: Dimensionality Reduction using PCA
from sklearn.decomposition import PCA
import time

# Set number of principal components (you can tune this number)
n_components = 100

# Start timing the PCA training
start_time = time.time()

# Fit PCA on the training data
pca = PCA(n_components=n_components, random_state=42)
x_train_pca = pca.fit_transform(x_train)

# End timing
pca_training_time = time.time() - start_time

# Transform validation and test data using the trained PCA model
x_val_pca = pca.transform(x_val)
x_test_pca = pca.transform(x_test)

# Print shapes and training time
print(f"PCA training time: {pca_training_time:.2f} seconds")
print(f"x_train_pca shape: {x_train_pca.shape}")
print(f"x_val_pca shape: {x_val_pca.shape}")
print(f"x_test_pca shape: {x_test_pca.shape}")

# Step 3b: Dimensionality Reduction using Stacked Autoencoder (SAE)
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping

import time

# Set encoding dimension (latent space size)
encoding_dim = 100  # You can change this number as needed

# Input layer
input_img = Input(shape=(784,))

# Encoder layers
encoded = Dense(512, activation='relu')(input_img)
encoded = Dense(256, activation='relu')(encoded)
encoded = Dense(encoding_dim, activation='relu')(encoded)

# Decoder layers
decoded = Dense(256, activation='relu')(encoded)
decoded = Dense(512, activation='relu')(decoded)
decoded = Dense(784, activation='sigmoid')(decoded)

# Autoencoder model
autoencoder = Model(inputs=input_img, outputs=decoded)

# Compile model
autoencoder.compile(optimizer=Adam(), loss='binary_crossentropy')

# Early stopping for better generalization
early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)

# Train the autoencoder and time it
start_time = time.time()
history = autoencoder.fit(
    x_train, x_train,
    epochs=50,
    batch_size=256,
    shuffle=True,
    validation_data=(x_val, x_val),
    callbacks=[early_stop],
    verbose=1
)
sae_training_time = time.time() - start_time

print(f"SAE training time: {sae_training_time:.2f} seconds")

# Step 3c: Extract encoder part of the autoencoder
encoder = Model(inputs=input_img, outputs=encoded)

# Use the encoder to transform the data
x_train_sae = encoder.predict(x_train)
x_val_sae = encoder.predict(x_val)
x_test_sae = encoder.predict(x_test)

# Print transformed shapes
print(f"x_train_sae shape: {x_train_sae.shape}")
print(f"x_val_sae shape: {x_val_sae.shape}")
print(f"x_test_sae shape: {x_test_sae.shape}")

# Step 4: Visualize original and reconstructed images (SAE only)
import matplotlib.pyplot as plt

# Get class names for fashion MNIST
class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',
               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']

# Select one image per class from the training set
samples = []
labels_seen = set()

for img, label in zip(x_train, y_train):
    if label not in labels_seen:
        samples.append((img, label))
        labels_seen.add(label)
    if len(samples) == 10:
        break

# Plot original and reconstructed images side by side
plt.figure(figsize=(20, 4))
for i, (img_flat, label) in enumerate(samples):
    # Reconstruct image using autoencoder
    reconstructed = autoencoder.predict(img_flat.reshape(1, 784))

    # Plot original
    ax = plt.subplot(2, 10, i + 1)
    plt.imshow(img_flat.reshape(28, 28), cmap="gray")
    plt.title(f"Original\n{class_names[label]}")
    plt.axis("off")

    # Plot reconstruction
    ax = plt.subplot(2, 10, i + 1 + 10)
    plt.imshow(reconstructed.reshape(28, 28), cmap="gray")
    plt.title("Reconstructed")
    plt.axis("off")

plt.suptitle("Original vs Reconstructed Images (SAE)", fontsize=16)
plt.tight_layout()
plt.show()

# Step 5a: Explained Variance plot for PCA
import numpy as np
import matplotlib.pyplot as plt

# Get cumulative explained variance
cumulative_variance = np.cumsum(pca.explained_variance_ratio_)

# Plot
plt.figure(figsize=(8, 4))
plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o')
plt.title("Cumulative Explained Variance by PCA Components")
plt.xlabel("Number of Components")
plt.ylabel("Cumulative Explained Variance")
plt.grid(True)
plt.tight_layout()
plt.show()

from sklearn.cluster import MiniBatchKMeans
import time

# Helper function to run MiniBatchKMeans and measure time
def run_minibatch_kmeans(data, n_clusters=10):
    start_time = time.time()
    kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, batch_size=256)
    labels = kmeans.fit_predict(data)
    duration = time.time() - start_time
    return labels, duration

# Clustering on raw test data
labels_raw, time_raw = run_minibatch_kmeans(x_test)

# Clustering on PCA-reduced test data
labels_pca, time_pca = run_minibatch_kmeans(x_test_pca)

# Clustering on SAE-reduced test data
labels_sae, time_sae = run_minibatch_kmeans(x_test_sae)

# Print results
print(f"MiniBatch KMeans on RAW: {time_raw:.2f} sec")
print(f"MiniBatch KMeans on PCA: {time_pca:.2f} sec")
print(f"MiniBatch KMeans on SAE: {time_sae:.2f} sec")

from sklearn.cluster import DBSCAN

# Helper function for DBSCAN with timing
def run_dbscan(data, eps=3.0, min_samples=5):
    start_time = time.time()
    dbscan = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=-1)
    labels = dbscan.fit_predict(data)
    duration = time.time() - start_time
    return labels, duration

# Run DBSCAN on PCA-reduced test data
labels_dbscan_pca, time_dbscan_pca = run_dbscan(x_test_pca, eps=3.0)

# Run DBSCAN on SAE-reduced test data
labels_dbscan_sae, time_dbscan_sae = run_dbscan(x_test_sae, eps=3.0)

# Print results
print(f"DBSCAN on PCA: {time_dbscan_pca:.2f} sec")
print(f"DBSCAN on SAE: {time_dbscan_sae:.2f} sec")

from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, silhouette_score

# Helper function to evaluate clustering results
def evaluate_clustering(X, labels, name):
    # Remove noise points (if any)
    mask = labels != -1
    X_filtered = X[mask]
    labels_filtered = labels[mask]

    n_clusters = len(set(labels_filtered))

    if len(set(labels_filtered)) < 2:
        print(f"{name}: Not enough clusters to evaluate.")
        return {
            'n_clusters': n_clusters,
            'calinski': None,
            'davies': None,
            'silhouette': None
        }

    scores = {
        'n_clusters': n_clusters,
        'calinski': calinski_harabasz_score(X_filtered, labels_filtered),
        'davies': davies_bouldin_score(X_filtered, labels_filtered),
        'silhouette': silhouette_score(X_filtered, labels_filtered)
    }

    print(f"{name} — Clusters: {scores['n_clusters']}, "
          f"CH: {scores['calinski']:.2f}, "
          f"DB: {scores['davies']:.2f}, "
          f"Silhouette: {scores['silhouette']:.2f}")

    return scores

# Evaluate all clustering results
results = {}

results['Raw + MiniBatchKMeans'] = evaluate_clustering(x_test, labels_raw, "Raw + MiniBatchKMeans")
results['PCA + MiniBatchKMeans'] = evaluate_clustering(x_test_pca, labels_pca, "PCA + MiniBatchKMeans")
results['SAE + MiniBatchKMeans'] = evaluate_clustering(x_test_sae, labels_sae, "SAE + MiniBatchKMeans")

results['PCA + DBSCAN'] = evaluate_clustering(x_test_pca, labels_dbscan_pca, "PCA + DBSCAN")
results['SAE + DBSCAN'] = evaluate_clustering(x_test_sae, labels_dbscan_sae, "SAE + DBSCAN")

import pandas as pd

# Collect all results in a list of dicts
summary = [
    {
        "DimRed": "Raw",
        "Clustering": "MiniBatchKMeans",
        "DimRedTime": 0.0,
        "ClusterTime": time_raw,
        "n_clusters": results['Raw + MiniBatchKMeans']['n_clusters'],
        "CH": results['Raw + MiniBatchKMeans']['calinski'],
        "DB": results['Raw + MiniBatchKMeans']['davies'],
        "Silhouette": results['Raw + MiniBatchKMeans']['silhouette']
    },
    {
        "DimRed": "PCA",
        "Clustering": "MiniBatchKMeans",
        "DimRedTime": pca_training_time,
        "ClusterTime": time_pca,
        "n_clusters": results['PCA + MiniBatchKMeans']['n_clusters'],
        "CH": results['PCA + MiniBatchKMeans']['calinski'],
        "DB": results['PCA + MiniBatchKMeans']['davies'],
        "Silhouette": results['PCA + MiniBatchKMeans']['silhouette']
    },
    {
        "DimRed": "SAE",
        "Clustering": "MiniBatchKMeans",
        "DimRedTime": sae_training_time,
        "ClusterTime": time_sae,
        "n_clusters": results['SAE + MiniBatchKMeans']['n_clusters'],
        "CH": results['SAE + MiniBatchKMeans']['calinski'],
        "DB": results['SAE + MiniBatchKMeans']['davies'],
        "Silhouette": results['SAE + MiniBatchKMeans']['silhouette']
    },
    {
        "DimRed": "PCA",
        "Clustering": "DBSCAN",
        "DimRedTime": pca_training_time,
        "ClusterTime": time_dbscan_pca,
        "n_clusters": results['PCA + DBSCAN']['n_clusters'],
        "CH": results['PCA + DBSCAN']['calinski'],
        "DB": results['PCA + DBSCAN']['davies'],
        "Silhouette": results['PCA + DBSCAN']['silhouette']
    },
    {
        "DimRed": "SAE",
        "Clustering": "DBSCAN",
        "DimRedTime": sae_training_time,
        "ClusterTime": time_dbscan_sae,
        "n_clusters": results['SAE + DBSCAN']['n_clusters'],
        "CH": results['SAE + DBSCAN']['calinski'],
        "DB": results['SAE + DBSCAN']['davies'],
        "Silhouette": results['SAE + DBSCAN']['silhouette']
    }
]

# Create and show DataFrame
df_results = pd.DataFrame(summary)
print(df_results)

import matplotlib.pyplot as plt
import numpy as np

# Choose 2 random cluster IDs (από 0 έως 9 γιατί έχουμε 10 clusters)
unique_clusters = np.unique(labels_pca)
chosen_clusters = np.random.choice(unique_clusters, size=2, replace=False)
print(f"Selected clusters: {chosen_clusters}")

# Plot 10 random images from each cluster
plt.figure(figsize=(20, 4))
for i, cluster_id in enumerate(chosen_clusters):
    # Find indices of test samples that belong to the current cluster
    cluster_indices = np.where(labels_pca == cluster_id)[0]

    # Choose 10 random indices from this cluster
    chosen_indices = np.random.choice(cluster_indices, size=10, replace=False)

    for j, idx in enumerate(chosen_indices):
        ax = plt.subplot(2, 10, i * 10 + j + 1)
        plt.imshow(x_test[idx].reshape(28, 28), cmap="gray")
        plt.title(f"Cluster {cluster_id}")
        plt.axis("off")

plt.suptitle("10 Random Images from 2 Selected Clusters (PCA + MiniBatchKMeans)", fontsize=16)
plt.tight_layout()
plt.show()